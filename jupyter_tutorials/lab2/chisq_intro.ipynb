{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to $\\chi^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import numpy as np, pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, and in science generally, we use measurements to select between theoretical models. So let us assume we made some measurements. Now what? What criterion should we use to prefer one model over another?\n",
    "\n",
    "A reasonable (but caveat: *incomplete*) position is to choose the model with the highest probability of yielding our measurements. That is, we want the **most likely** model given the data. That is where this tutorial begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why $\\chi^2$?\n",
    "\n",
    "Suppose we are looking for the likeliest (maximum likelihood) model, given our data.\n",
    "\n",
    "What determines the likelihood of a model?  This question is hard to answer head-on, so scientists\n",
    "instead answer a subtly different (but closely related; see Bayes' Theorem) question: What is the likelihood,\n",
    "given a model, that we would measure *these* data?\n",
    "\n",
    "This likelihood, $L$, is the probability \n",
    "that each measurement's deviation from the model happened by random chance.  If we assume\n",
    "Gaussian statistics (a reasonable assumption for a wide range of situations; see the Central Limit Theorem),\n",
    "we may write the likelihood of the data, $y$, given a model, $f$ as\n",
    "\\begin{equation}\n",
    "L = \\prod_i\\frac{1}{\\sigma_i\\sqrt{2\\pi}}\n",
    "e^{-\\left|y_i-f(\\vec x_i,\\vec p)\\right|^2/2\\sigma_i^2}.\n",
    "\\end{equation}\n",
    "Notice how our model $f$ accepts a set of coordinates $\\vec x_i$ describing the configuration used when we\n",
    "measured $y_i$, as well as a set of parameters $\\vec p$ that might be tuned to better fit our data.\n",
    "Don't worry about the $1/\\sigma_i\\sqrt{2\\pi}$ prefactor; that is just a normalization that ensures probabilities add to one.  The important part is the exponential.\n",
    "\n",
    "To find the maximum likelihood, we need to maximize $L$, but exponentials are cumbersome. \n",
    "Instead, let us maximize $\\log L$:\n",
    "\\begin{equation}\n",
    "\\log L = -\\frac12\\sum_i\\frac{\\left|y_i-f(\\vec x_i,\\vec p)\\right|^2}{\\sigma_i^2} - \\sum_i \\log(\\sigma_i\\sqrt{2\\pi})\n",
    "\\end{equation}\n",
    "The second term (with the $\\sigma_i\\sqrt{2\\pi}$) is a constant that doesn't depend on the model, so we \n",
    "will ignore it.  Instead, let us focus on optimizing the first term. To clean things up, we drop the $-\\frac12$ factor (so now we are *minimizing*), to get\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p)|^2}{\\sigma_i^2}}.\n",
    "\\end{equation}\n",
    "\n",
    "Maximizing $L$ is equivalent to minimizing $\\chi^2$.\n",
    "Moreover, since $L\\propto e^{-\\frac12\\chi^2}$, $\\chi^2$ is a measure of the number of \"sigmas\" (squared) by which the fit deviates from the model.\n",
    "\n",
    "So to summarize, minimizing $\\chi^2$ is equivalent to maximizing the likelihood of a model under the assumption that errors are Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Fitting a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a linear function,\n",
    "\\begin{equation}\n",
    "f(x,m,b)=mx+b,\n",
    "\\end{equation}\n",
    "that we have measured at various $x_i$'s.  Our measurements are noisy, so for each measurement, $y_i$, \n",
    "we get a sample of the function $f(x_i,m,b)$ with added noise, $n_i$:\n",
    "\\begin{equation}\n",
    "y_i=f(x_i,m,b)+n_i,\n",
    "\\end{equation}\n",
    "Let us assume the noise is drawn from a Gaussian distribution and each measurement has statistically identical noise with variance $\\langle n_i^2\\rangle\\equiv\\sigma^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,m,b):\n",
    "    return m*x+b\n",
    "\n",
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given the measurements, `ys`, at the coordinates specified in `xs`, what is your best guess for ($m$,$b$)?\n",
    "- (1,1)\n",
    "- (2,1)\n",
    "- (1,2)\n",
    "- (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a menu of choices, you can directly evaluate $\\chi^2$ for each one:\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p)|^2}{\\sigma_i^2}}.\n",
    "\\end{equation}\n",
    "In the above equation, $i$ indexes different measurements, $\\sigma_i$ is the expected error in each measurment, and we have introduced a vector, $\\vec p$, that contains the parameters we are fitting.  In the above example, $\\vec p = (m, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square: [13.61  3.14 20.85 10.39]\n"
     ]
    }
   ],
   "source": [
    "def chisq(ys, mdl, sig):\n",
    "    return np.sum(np.abs(ys-mdl)**2/sig**2)\n",
    "\n",
    "sig = 1. # Let's assume \\sigma_i^2=1 for now\n",
    "ps = [(1,1),(2,1),(1,2),(2,2)]\n",
    "X2 = [chisq(ys, f(xs,*p), sig) for p in ps]\n",
    "print('Chi-Square:', np.around(X2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears $(m,b)=(2,1)$ is the best answer! Here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIiAPx/Mgb1weugDhcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(xs,ys,yerr=1, color='k', linestyle='')\n",
    "plt.plot(xs, 2*xs+1, 'k:')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens if we don't have a menu of choices?  Our options are:\n",
    "- make our own menu (e.g. sample a grid at some resolution), or\n",
    "- optimize!\n",
    "\n",
    "It's time to take a trip down the rabbit-hole of optimizing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing $\\chi^2$\n",
    "\n",
    "Optimizing (which in numerics is almost always framed as minimizing) a general function can be done by looping over the following four steps:\n",
    "1. Start somewhere.  Choose a $\\vec p_j$ (the $j^{\\rm th}$ successive estimate of the optimal parameters, $\\vec p$) and evaluate\n",
    "\\begin{equation}\n",
    "\\chi_j^2=\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p_j)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}\n",
    "2. Compute the gradient (slope) of $\\chi^2$ w.r.t. $p_{j,a},p_{j,b},\\dots=\\vec p_j$ to get $\\frac{\\partial\\chi^2}{\\partial \\vec p_j}$\n",
    "3. Check to see if either $\\chi_j^2$ or $\\frac{\\partial\\chi^2}{\\partial \\vec p_j}$ are small enough to declare victory.  If so, STOP.\n",
    "4. Otherwise, take a step \"downhill\" toward your target $\\chi^2_T$ and return to step 1:\n",
    "\\begin{align}\n",
    "\\frac{\\chi_T^2-\\chi_j^2}{\\vec p_{j+1}-\\vec p_j}&=-\\frac{\\partial\\chi^2}{\\partial \\vec p_j}\\\\\n",
    "\\vec p_{j+1} &= \\vec p_j - \\frac{\\chi_T^2-\\chi_j^2}{\\partial\\chi^2/\\partial \\vec p_j}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a Gradient Numerically\n",
    "\n",
    "Suppose you have a general function $g(\\vec x)$ that you need to take the gradient of:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial g}{\\partial\\vec x}=\\left(\\frac{\\partial g}{\\partial x_0}, \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\dots\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Numerically, this might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(x) = 33.0\n",
      "dg/dx = [ 2. 64. -1.]\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return x[0]**2 + 2*x[1]**4 + np.cos(x[2])\n",
    "\n",
    "x0,x1,x2 = x = np.array([1.,2,np.pi/2])\n",
    "g0 = g(x)\n",
    "#dx = 1e-15 # errors from numerical precision\n",
    "dx = 1e-5 # good?\n",
    "#dx = 1e-1 # errors from step size\n",
    "x0_step = np.array([x0+dx,x1,x2])\n",
    "x1_step = np.array([x0,x1+dx,x2])\n",
    "x2_step = np.array([x0,x1,x2+dx])\n",
    "dg_dx = np.array([(g(x0_step)-g0)/dx, (g(x1_step)-g0)/dx, (g(x2_step)-g0)/dx])\n",
    "print('g(x) =', g0)\n",
    "print('dg/dx =', np.around(dg_dx, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate along, computing gradients, stepping downhill, and recomputing, we can eventually find where (or if) the function's gradient goes to zero, indicating we've minimized it.  However, there is something of an art to taking steps of the right size such that:\n",
    "- you don't spend forever getting there, but\n",
    "- you don't overstep, go unstable, and end up at infinity.\n",
    "\n",
    "Here's an example of a minimizing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize(func, xs, stepsize=1e-2, dx=1e-5, maxiter=100, tol=1e-3):\n",
    "    '''Minimize a function using a gradient-descent algorithm.'''\n",
    "    for iter in range(maxiter):\n",
    "        f0 = func(xs)\n",
    "        ndims = len(xs)\n",
    "        x_steps = [xs.copy() for i in range(ndims)]\n",
    "        for i in xrange(ndims):\n",
    "            x_steps[i][i] += dx\n",
    "        grad = np.array([(func(x_steps[i])-f0)/dx for i in range(ndims)])\n",
    "        if np.abs(grad).max() < tol:\n",
    "            break\n",
    "        xs = np.array([xs[i]-stepsize*grad[i] for i in range(ndims)])\n",
    "    return {'iter':iter, 'func':f0, 'grad':grad, 'x':xs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [-0.    0.05  3.14]\n",
      "Iterations: 509\n"
     ]
    }
   ],
   "source": [
    "result = minimize(g, np.array([1.,2,3]), stepsize=.03, maxiter=1000)\n",
    "print('Solution:', np.around(result['x'],2))\n",
    "print('Iterations:', result['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [0.14 0.25 3.09]\n",
      "Iterations: 999\n"
     ]
    }
   ],
   "source": [
    "result = minimize(g, np.array([1.,2,3]), stepsize=.001, maxiter=1000)\n",
    "print('Solution:', np.around(result['x'],2))\n",
    "print('Iterations:', result['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [5.10000000e-01 7.11588315e+15 3.04000000e+00]\n",
      "Iterations: 4\n"
     ]
    }
   ],
   "source": [
    "result = minimize(g, np.array([1.,2,3]), stepsize=.1, maxiter=1000)\n",
    "print('Solution:', np.around(result['x'],2))\n",
    "print('Iterations:', result['iter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our ability to converge to a reasonable solution can be finicky. Fortunately, there are libraries for this kind of thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1.000000\n",
      "         Iterations: 146\n",
      "         Function evaluations: 259\n",
      "Solution: [-0.   -0.    3.14]\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "result = scipy.optimize.fmin(g, np.array([1.,2,3]))\n",
    "print('Solution:', np.around(result,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Optimizing $\\chi^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.883578\n",
      "         Iterations: 35\n",
      "         Function evaluations: 67\n",
      "m,b = [2.02 0.78]\n"
     ]
    }
   ],
   "source": [
    "def chisq_min(p):\n",
    "    m,b = p\n",
    "    return chisq(ys, f(xs,m,b), 1.) \n",
    "\n",
    "ans = scipy.optimize.fmin(chisq_min, np.array([.5,.5]))\n",
    "print('m,b =', np.around(ans,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've gone and fit `m` and `b` in our `f(x) = m*x+b` example, and now we have an answer.  Is this the actual answer?  What are the statistical errors on these measurements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you were wondering, the above is what `np.polyfit` does, except it's programmed in the gradients analytically, so it converges faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m,b = 2.0232641269999996 0.7755130020000003\n"
     ]
    }
   ],
   "source": [
    "m,b = np.polyfit(xs,ys, deg=1)\n",
    "print('m,b =', m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJwj9tpLIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116bf2090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs,ys,'k.')\n",
    "plt.plot(xs, ans[0]*xs+ans[1], 'r:')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Errors\n",
    "\n",
    "In the above example, it looks like the final $\\chi^2$ of our fit was 2.8.  Is this good?\n",
    "- Yes\n",
    "- No\n",
    "- Maybe\n",
    "- Depends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the way we've been writing $\\chi^2$, it scales with the number of measurements we make:\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "To make a fair comparison, we need to normalize this to the error we expect in a \"perfect\" fit.  Suppose we knew $m,b$ perfectly, so that we could subtract $f(x_i,m,b)$ from our measurement $y_i=f(x_i,m,b)+n_i$.  In this case, the residual would be $n_i$, and $\\langle|n_i|^2\\rangle=\\sigma_i^2$.  If we know our $\\sigma^2$ well, we can construct a *reduced* $\\chi^2$,\n",
    "\\begin{equation}\n",
    "\\chi_r^2=\\frac{1}{N}\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}},\n",
    "\\end{equation}\n",
    "where we expect $\\chi_r^2\\approx1$ if we know $\\sigma_i$ well enough.\n",
    "\n",
    "So above, we had $N=5$ measurements, to $\\chi_r^2=\\chi^2/5=0.58$.  That's a good fit.  Perhaps too good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Fits are \"Too Good to be True\"\n",
    "\n",
    "Suppose I have a silly line, $y=0$, and I add some noise to a bunch of measurements of this line, and then fit a line to those measurements and ask for my reduced $\\chi^2$ with perfect knowledge of my noise statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980758577942968\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    m,b = np.polyfit(xs, ys, deg=1)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval([m,b], xs))**2) / ys.size)\n",
    "print(np.average(chisq_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is $\\chi_r^2<1$?\n",
    "- The line ate the noise\n",
    "- Need better noise model\n",
    "- There wasn't actually a line\n",
    "- Dividing by wrong number of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a hint, here's the same code, fitting a 4th order (5 parameter) polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5030894745224488\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    poly = np.polyfit(xs, ys, deg=4)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval(poly, xs))**2) / ys.size)\n",
    "print(np.average(chisq_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the degrees of freedom in your model give it enough wiggle room to absorb noise.  Think if it this way: if I fit a line to 2 noisy points, I'll get a perfect fit ($\\chi^2=0$), but that doesn't mean that my answer is perfect.\n",
    "\n",
    "In fact, for every degree of freedom in our model, we can absorb out a degree of freedom in our data.  That means that when we thought we had 10 independent measurements, we actually only had 8 (or 5 in the `deg=4` case).  To account for this, the *real* definition of the reduced $\\chi^2$ is:\n",
    "\\begin{equation}\n",
    "\\chi_r^2=\\frac{1}{N-M}\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p)|^2}{\\sigma_i^2}},\n",
    "\\end{equation}\n",
    "where $M$ is the number of degrees of freedom in the model.  Those degrees of freedom absorb out noise in the data that will then not contribute to the $\\chi^2$.  These degrees of freedom also mean that your model is not as over-constrained as you might otherwise think.  To do good science, we want our model to be highly over-constrained, so that errors beat down as $\\sqrt{N-M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9927387234221549\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    poly = np.polyfit(xs, ys, deg=4)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval(poly, xs))**2) / (ys.size-len(poly)))\n",
    "print(np.average(chisq_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating $\\chi^2$ to Parameter Errors\n",
    "\n",
    "The final step here is to use our $\\chi^2$ to estimate the errors on our parameters.  Why is there error in our parameters, and how did it get there?  Well, remember when we said that the degrees of freedom in our model absorbed out some of the noise?\n",
    "\n",
    "Yep.  That's how.\n",
    "\n",
    "How much did it absorb?  Apparently the difference between $N$ and $N-M$.  A \"perfect\" fit should have had a (non-reduced) $\\chi^2$ that was $M$ higher. That means our $\\chi^2$ (again, *not* reduced) should be allowed to increase by 1 for each parameter we fit to get 1$\\sigma$ error bars (because $\\chi^2$ is in units of $\\sigma$).\n",
    "We change $\\vec p$ until $\\chi^2$ increases by 1 (for 1$\\sigma$ error bars), 4 (for 2$\\sigma$ error bars), or 9 (for 3$\\sigma$ error bars).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0237900858322155\n",
      "1.0242099141677845\n",
      "1.0124092932583366\n",
      "1.0125907067416637\n"
     ]
    }
   ],
   "source": [
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])\n",
    "\n",
    "def chisq_min(p):\n",
    "    m,b = p\n",
    "    return chisq(ys, f(xs,m,b), 1.)\n",
    "\n",
    "chi0 = chisq_min(ans)\n",
    "print(chisq_min(ans+np.array([.32,0])) - chi0)\n",
    "print(chisq_min(ans+np.array([-.32,0])) - chi0)\n",
    "print(chisq_min(ans+np.array([0,0.45])) - chi0)\n",
    "print(chisq_min(ans+np.array([0,-0.45])) - chi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 2.023231 +/- 0.320000\n",
      "b = 0.775493 +/- 0.450000\n"
     ]
    }
   ],
   "source": [
    "print('m = %f +/- %f' % (ans[0], .32))\n",
    "print('b = %f +/- %f' % (ans[1], .45))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
