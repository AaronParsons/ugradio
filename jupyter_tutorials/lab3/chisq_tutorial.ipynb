{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A $\\chi^2$ Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import numpy as np, pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a linear function,\n",
    "\\begin{equation}\n",
    "f(x,m,b)=mx+b,\n",
    "\\end{equation}\n",
    "that we have measured at various $x_i$'s.  Our measurements are noisy, so for each measurement, $y_i$, that we make, \n",
    "we get a sample of the function $f(x_i,m,b)$ added with some noise, $n_i$:\n",
    "\\begin{equation}\n",
    "y_i=f(x_i,m,b)+n_i,\n",
    "\\end{equation}\n",
    "For this example, let's assume the noise is drawn from a Gaussian distribution and each measurement has statistically identical noise with a variance of $\\langle n_i^2\\rangle\\equiv\\sigma^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,m,b):\n",
    "    return m*x+b\n",
    "\n",
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given the measurements, `ys`, at the coordinates specified in `xs`, which would be your best guess for ($m$,$b$)?\n",
    "- (1,1)\n",
    "- (2,1)\n",
    "- (1,2)\n",
    "- (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a menu of choices, just evaluate $\\chi^2$ for each one:\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p)|^2}{\\sigma_i^2}}.\n",
    "\\end{equation}\n",
    "In the above equation, $i$ indexes different measurements, $\\sigma_i$ is the expected error in each measurment, and we have introduced a vector, $\\vec p$, that contains the parameters we are fitting.  In the above example, $\\vec p = (m, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square: [13.61  3.14 20.85 10.39]\n"
     ]
    }
   ],
   "source": [
    "def chisq(ys, mdl, sig):\n",
    "    return np.sum(np.abs(ys-mdl)**2/sig**2)\n",
    "\n",
    "sig = 1. # Let's assume \\sigma_i^2=1 for now\n",
    "ps = [(1,1),(2,1),(1,2),(2,2)]\n",
    "X2 = [chisq(ys, f(xs,*p), sig) for p in ps]\n",
    "print('Chi-Square:', np.around(X2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears $(m,b)=(2,1)$ is the best answer! Here's what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAWz0zSJ3eW7SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11379a9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs,ys,'k.')\n",
    "plt.plot(xs, 2*xs+1, 'k:')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens if we don't have a menu of choices?  Our options are:\n",
    "- make our own menu (e.g. sample a grid at some resolution), or\n",
    "- optimize!\n",
    "\n",
    "In a moment, we will take a trip down the rabbit-hole of optimizing functions, but first, let's ask the reasonable question, \"Why $X^2$?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why $\\chi^2$?\n",
    "\n",
    "When fitting a model to data, we are looking for the likeliest (maximum likelihood) solution.  What determines the likelihood, $L$, of a solution?  It is the probability \n",
    "that each measurement's deviation from the model could happen by random chance.  Mathematically, this is written\n",
    "\\begin{equation}\n",
    "L = \\prod_i\\frac{1}{\\sigma_i\\sqrt{2\\pi}}\n",
    "e^{-\\left|y_i-f(\\vec x_i,\\vec p)\\right|^2/2\\sigma_i^2}.\n",
    "\\end{equation}\n",
    "Don't worry about the $1/\\sigma_i\\sqrt{2\\pi}$ prefactor; that's just a normalization that ensures probabilities add to one.  The important part is the exponential.\n",
    "\n",
    "We need to maximize $L$, but exponentials are cumbersome. \n",
    "Instead, let's maximize $\\log L$:\n",
    "\\begin{equation}\n",
    "\\log L = -\\frac12\\sum_i\\frac{\\left|y_i-f(\\vec x_i,\\vec p)\\right|^2}{\\sigma_i^2} - \\sum_i \\log(\\sigma_i\\sqrt{2\\pi})\n",
    "\\end{equation}\n",
    "The second term (with the $\\sigma_i\\sqrt{2\\pi}$) doesn't depend on the model, so we can safely ignore it---it's a constant.  Let's focus on optimizing the first term. To clean things up, we'll drop the $-\\frac12$ factor (so now we are *minimizing*), which gives us\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p)|^2}{\\sigma_i^2}}.\n",
    "\\end{equation}\n",
    "\n",
    "Maximizing $L$ is the same as minimizing $\\chi^2$.\n",
    "Moreover, since $L\\propto e^{-\\frac12\\chi^2}$, $\\chi^2$ is a measure of the number of \"sigmas\" (squared) by which your fit deviates from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing $\\chi^2$\n",
    "\n",
    "Optimizing (which in numerics is almost always framed as minimizing) a general function can be done by looping over the following four steps:\n",
    "1. Start somewhere.  Choose a $\\vec p_j$ (the $j^{\\rm th}$ successive estimate of the optimal parameters, $\\vec p$) and evaluate\n",
    "\\begin{equation}\n",
    "\\chi_j^2=\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p_j)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}\n",
    "2. Compute the gradient (slope) of $\\chi^2$ w.r.t. $p_A,p_B,\\dots=\\vec p_j$ to get $\\frac{\\partial\\chi^2}{\\partial \\vec p_j}$\n",
    "3. Check to see if either $\\chi_j^2$ or $\\frac{\\partial\\chi^2}{\\partial \\vec p_j}$ are small enough to declare victory.  If so, STOP.\n",
    "4. Otherwise, take a step \"downhill\" toward your target $\\chi^2_T$ and return to step 1:\n",
    "\\begin{align}\n",
    "\\frac{\\chi_T^2-\\chi_j^2}{\\vec p_{j+1}-\\vec p_j}&=-\\frac{\\partial\\chi^2}{\\partial \\vec p_j}\\\\\n",
    "\\vec p_{j+1} &= \\vec p_j - \\frac{\\chi_T^2-\\chi_j^2}{\\partial\\chi^2/\\partial \\vec p_j}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a Gradient Numerically\n",
    "\n",
    "Suppose you have a general function $g(\\vec x)$ that you need to take the gradient of:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial g}{\\partial\\vec x}=\\left(\\frac{\\partial g}{\\partial x_0}, \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\dots\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Numerically, this might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(x) = 33.0\n",
      "dg/dx = [ 2. 64. -1.]\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return x[0]**2 + 2*x[1]**4 + np.cos(x[2])\n",
    "\n",
    "x0,x1,x2 = x = np.array([1.,2,np.pi/2])\n",
    "g0 = g(x)\n",
    "#dx = 1e-15 # errors from numerical precision\n",
    "dx = 1e-5 # good?\n",
    "#dx = 1e-1 # errors from step size\n",
    "x0_step = np.array([x0+dx,x1,x2])\n",
    "x1_step = np.array([x0,x1+dx,x2])\n",
    "x2_step = np.array([x0,x1,x2+dx])\n",
    "dg_dx = np.array([(g(x0_step)-g0)/dx, (g(x1_step)-g0)/dx, (g(x2_step)-g0)/dx])\n",
    "print('g(x) =', g0)\n",
    "print('dg/dx =', np.around(dg_dx, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate along, computing gradients, stepping downhill, and recomputing, we can eventually find where (or if) the function's gradient goes to zero, indicating we've minimized it.  However, there is something of an art to taking steps of the right size such that:\n",
    "- you don't spend forever getting there, but\n",
    "- you don't overstep, go unstable, and end up at infinity.\n",
    "\n",
    "Here's an example of a minimizing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize(func, xs, stepsize=1e-2, dx=1e-5, maxiter=100, tol=1e-3):\n",
    "    '''Minimize a function using a gradient-descent algorithm.'''\n",
    "    for iter in range(maxiter):\n",
    "        f0 = func(xs)\n",
    "        ndims = len(xs)\n",
    "        x_steps = [xs.copy() for i in range(ndims)]\n",
    "        for i in xrange(ndims):\n",
    "            x_steps[i][i] += dx\n",
    "        grad = np.array([(func(x_steps[i])-f0)/dx for i in range(ndims)])\n",
    "        if np.abs(grad).max() < tol:\n",
    "            break\n",
    "        xs = np.array([xs[i]-stepsize*grad[i] for i in range(ndims)])\n",
    "    return {'iter':iter, 'func':f0, 'grad':grad, 'x':xs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [-0.    0.05  3.14]\n",
      "Iterations: 509\n"
     ]
    }
   ],
   "source": [
    "result = minimize(g, np.array([1.,2,3]), stepsize=.03, maxiter=1000)\n",
    "print('Solution:', np.around(result['x'],2))\n",
    "print('Iterations:', result['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [0.14 0.25 3.09]\n",
      "Iterations: 999\n"
     ]
    }
   ],
   "source": [
    "result = minimize(g, np.array([1.,2,3]), stepsize=.001, maxiter=1000)\n",
    "print('Solution:', np.around(result['x'],2))\n",
    "print('Iterations:', result['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [5.10000000e-01 7.11588315e+15 3.04000000e+00]\n",
      "Iterations: 4\n"
     ]
    }
   ],
   "source": [
    "result = minimize(g, np.array([1.,2,3]), stepsize=.1, maxiter=1000)\n",
    "print('Solution:', np.around(result['x'],2))\n",
    "print('Iterations:', result['iter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our ability to converge to a reasonable solution can be finicky. Fortunately, there are libraries for this kind of thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1.000000\n",
      "         Iterations: 146\n",
      "         Function evaluations: 259\n",
      "Solution: [-0.   -0.    3.14]\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "result = scipy.optimize.fmin(g, np.array([1.,2,3]))\n",
    "print('Solution:', np.around(result,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Optimizing $\\chi^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.883578\n",
      "         Iterations: 35\n",
      "         Function evaluations: 67\n",
      "m,b = [2.02 0.78]\n"
     ]
    }
   ],
   "source": [
    "def chisq_min(p):\n",
    "    m,b = p\n",
    "    return chisq(ys, f(xs,m,b), 1.) \n",
    "\n",
    "ans = scipy.optimize.fmin(chisq_min, np.array([.5,.5]))\n",
    "print('m,b =', np.around(ans,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've gone and fit `m` and `b` in our `f(x) = m*x+b` example, and now we have an answer.  Is this the actual answer?  What are the statistical errors on these measurements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you were wondering, the above is what `np.polyfit` does, except it's programmed in the gradients analytically, so it converges faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m,b = 2.0232641269999996 0.7755130020000003\n"
     ]
    }
   ],
   "source": [
    "m,b = np.polyfit(xs,ys, deg=1)\n",
    "print('m,b =', m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHDRJREFUeJzt3Xt0VeWd//H3N4TDLShXAwIKWlS8VYy3qKMJ0S68TNWq\n46W1KCjLKjN2qh3tsOyasUWtHeul1vFKtTNorKg/0MEqhqTYEqRAEURQEVQQrYiIBOQcQp7fH89J\nQczl3Pc5+3xea52V/eTss/cn2/D1ybP3frY55xARkfAoCTqAiIhklgq7iEjIqLCLiISMCruISMio\nsIuIhIwKu4hIyKiwi4iEjAq7iEjIqLCLiIRMaRA7HTBggBs+fHhKn926dSu9evXKbKAMUK7kKFdy\nlCs5+ZoL0su2aNGiT51zAztd0TmX81dFRYVLVX19fcqfzSblSo5yJUe5kpOvuZxLLxuw0CVQYzUU\nIyISMirsIiIho8IuIhIyKuwiIiGjwi4iEjIq7CIiIaPCLiISMirsIiIho8IuIpJNzc2wYUNOdxnI\nlAIiIkXBOaiuhp494Q9/ALOc7FaFXUQk0zZsgIEDfSGfOBF6987p7jUUIyKSSfPnw/Dh8OKLvn3Z\nZXDuuTnrrYMKu4hIZnz2mf969NEwYQKMGhVYFBV2EZF0XXstnHwy7NgBkQjce6/vtQdEY+wiIqnY\nuhW6dYPSUjj7bDjggKAT/Z167CIiyfrwQzj4YHjwQd8+4wy4/nro2jXYXHEq7CIiiWpq8l/33RfO\nO8+Pp+chFXYRkUTcfz+MHAmbNvkrXH79a6isDDpVm1TYRUTa09IC27f75RNPhO98J6eXLaZKhV1E\npC3RqC/mN9/s20cdBb/5DfTpE2yuBKiwi4jsrrWH3q0bjBmTt+PoHVFhFxFp9fLLsP/+sGqVb996\nK1xySbCZUqDCLiISi/mvRxzhbzQqKezSmLEblMysC7AQ+NA5d3amtisiklVXXAFbtsD06TB4MDzz\nTNCJ0pbJO0+vA1YAe2VwmyIimdfc7O8YBTj8cH8XaUtLwffUW2XkpzCzocBZwCOZ2J6ISNasWAGH\nHAJz5/r29dfDT38amqIOmRtjvxv4N6AlQ9sTEcmsnTv91/339zcalYZ3qixzzqW3AbOzgTOdc9eY\nWRVwQ1tj7GY2EZgIUF5eXlFbW5vS/pqamigrK0sjcXYoV3KUKznKlZw9cw176ikGvPoqf73nHujS\nJcBk6R2z6urqRc65Yzpd0TmX1gu4DVgHvAd8DGwD/rejz1RUVLhU1dfXp/zZbFKu5ChXcpQrOfX1\n9c61tPiXc8498YRz48Y5t2VLkLHcvHnz3JVXXunmzZuX0ueBhS6Bupz2UIxz7ifOuaHOueHAxcAc\n59z30t2uiEiqSjdv9s8a/d3v/DcuuQQeewwC/OuisbGRmpoapk6dSk1NDY2NjVnbV3jOFohITjU2\nNjJt2rSsFqikxYeWm3v39kU8T6bRBWhoaCAWi9HS0kIsFqOhoSFr+8poYXfONThdwy4SernsfSZs\nxgx/+39Tk7/C5YUX4NJLg071d1VVVUQiEUpKSohEIlRVVWVtX+qxi0jSctn77FTrBSADB8Jee+16\n9mieqayspK6ujvHjx1NXV0dlFqf8De/1PiKSNa29z2g0mvXeZ7uam+Gqq2DECH8d+oknQkODn1Z3\n9erc50lAZWUl0Wg0q0Ud1GMXkRTksvf5Na099NJSf216y263zxTAXOm5oB67iKQkV73Pr1i8GH7w\nAz+fy9Ch8PjjKuZtUI9dRApH375+Xpf1631bRb1N6rGLSH674w5Yswb++7/9ePqyZSronVCPXUTy\n26ZN/kqX5mbfVlHvlAq7iOSXDz+Es86ChQt9e8oUeOqpUE/alWkq7CKSX8rK/KPp1qzx7RBNp5sr\nOmIiErznnvPzuTgHe+8Nb74JF14YdKqCpcIuIsHbsAHeeQc2bvTtgKfWLXQq7CKSe1u3wqRJvqcO\ncOWV8NprMGBAsLlCQoVdRHKvWzeYN88PuYAfR1cvPWNU2EUkNxYu9OPm27f7K1zmz4fJk4NOFUoq\n7CKSG5s2QWOjv+IFIBIJNk+I6cJQEcmOlha4+27o0cPP73L66b6od+8edLLQU49dRLKjpATmzIG5\nc3d9T0U9J1TYRSRz1q6F73/fX74I8Pvfw5NPBpupCKmwi0jmNDXBzJmwYIFv9+wZbJ4ipTF2EUnP\nzJl+xsXJk2HUKN9r79076FRFTT12EUnP7Nl+yCUa9W0V9cCpsItIcpqa4MYbd91cdPvt/hr1bt2C\nzSV/p6EYEUnO9u3wyCMweDAcdRT06hV0ItmDeuwi0rklS+Df/93PvjhggJ+w64c/DDqVtEOFXUQ6\nN2cOPPzwrmeN9usXbB7pkAq7iHzdzp3w4INQV+fb//zP8PbbMGRIsLkkISrsIvJ1zc1w553wxBO+\n3bUr9O0bbCZJmAq7iHgff+zH0Zub/RUuc+f6k6RScFTYRcSbP9/30lvvGh00CMyCzSQp0eWOIsVs\n9mz4/HM/T/o55/jZF4cNCzqVpEmFXaRYOQdTpvjr0i+4wPfOVdRDQUMxIsXkyy/9naKff+4L+RNP\nQEODhlxCRoVdpJisXOkn65oxw7f33VdzpIeQhmJEwm7lSn9i9PLLYfRo3x45MuhUkkXqsYuE3S9/\n6Sft2rrVt1XUQy/twm5mw8ys3sxWmNlyM7suE8FEJEXOwf/8D6xe7du/+IWfL12TdRWNTPTYm4Hr\nnXOjgBOAa83s0AxsV0RS8be/+YdHP/CAbw8YAPvsE2wmyam0C7tz7iPn3OL48hZgBaAJJURyaePG\nXXeJDhoEjY3+6hcpShkdYzez4cBo4LVMbldEOvHQQ3D11f4GI4AjjoASnUIrVuacy8yGzMqAPwJT\nnHPPtvH+RGAiQHl5eUVtbW1K+2lqaqKsrCydqFmhXMlRruS0lWuvZctwkQhbDj6YkmiUHuvXs3XE\niMBz5YN8zQXpZauurl7knDum0xWdc2m/gK7AS8CPElm/oqLCpaq+vj7lz2aTciVHuZLztVyxmHP7\n7efc2WcHkqdVwRyvPJJONmChS6DGZuKqGAMeBVY4536V7vZEpB2xGDz2GLS0+Gl0n38eUvzLV8It\nE4NwJwGXAWPMbEn8dWYGtisiu5s5E664Al5+2bePPFKXMEqb0r7z1Dn3J0ATTYhkw5o18P77fvn8\n8+GPf4RTTgk2k+Q9nTYXyWeXXw4TJ/rhFzMVdUmICrtIPnHOD7m03v7/wAP+QdK6dFGSoN8WkXyy\ndKl/4MXDD/v2qFEwdGiwmaTgqLCLBO2LL+CVV/zyN78JL74IkyYFm0kKmgq7SNBuuAHOPRc2bfLt\nsWOhVDNqS+pU2EWCsHgxrF/vl2++GerroW/fYDNJaKiwi+TaZ5/BySfDz37m28OGwbHHBptJQkWF\nXSQXdu70V7cA9OsH06fDbbcFm0lCS4VdJBfuvhtqavwDLwDOPBP69Ak2k4SWztCIZMv69f569JEj\n/U1Gw4fD4YcHnUqKgAq7SDbs3An/8A++mNfVQe/efkoAkRxQYRfJpMZGOOEE6NLF3zV6wAFBJ5Ii\npDF2kUz5v/+DE0+EGTN8+/TT4cADg80kRUmFXSQd27bBm2/65bFj/SPqzjgj2ExS9DQUI5KOCy6A\nlSvhrbf8wy+uuiroRCIq7CJJW7kSRoyAbt38XaOxmC/qInlCQzEicY2NjUybNo3Gxsb2V3r7bTji\nCLj3Xt+urIRTT81NQJEEqbCL4It6TU0NU6dOpaam5qvFvaXF99IBDjoI7rnHPwBDJE+psIsADQ0N\nxGIxWlpaiMViNDQ07Hrzxhvh+ONhwwbfvuYaGDgwkJwiidAYuwhQVVVFJBIhGo0SiUQ47aij/DS6\nffv6E6KHHQb9+wcdUyQh6rGLAJWVldTV1TF+/Hjqn3+eY8eNg5/8xL950EF+6EWPp5MCoR67SFzl\noEFEv/tdjq+qgp//HE46KehIIilRF0QE4Le/hZEj6bV6tW9PnOiHX0QKkHrsUryiUfj8cygv9w+Q\nXreOL/fdN+hUImlTj12Kk3Nwyilw2WV+uV8/uPlmWrp3DzqZSNrUY5fi8tFHMHgwmMG//Iu/0sUs\n6FQiGaUeuxSPP/3JTwUwa5Zvf/e7fuIukZBRYZdwc27XjUXHHQeTJsE3vxlsJpEsU2GXcLvqKj+X\ny44dEInAf/0XDBkSdCqRrNIYu4TP5s3QqxeUlvrH0R19tG4ukqKi33YJl7Vr/Z2iDz7o22ec4ed2\n6dIl2FwiOaTCLuGwebP/OnQofO97fjpdkSKlwi6F7+67fS990yZ/6eKdd/rhF5EipTF2KUzNzf6E\naI8eUF3th2BK9essAuqxSyHavh2OPdY/lg785Yt33gm9ewebSyRPqLBL4di2zX/t3h3+8R81+6JI\nOzJS2M1srJm9ZWarzOymTGxT5CteeAGGDYNVq3z7llvgvPOCzSSSp9Iu7GbWBfgNcAZwKHCJmR2a\n7nZFAD/sAlBRAd/6lr/JSEQ6lImzTccBq5xzqwHMrBY4B3gzA9uWYuUcXHopxGLwzDN+4q4nnww6\nlUhBMOdcehswuwAY65y7Mt6+DDjeOTdpj/UmAhMBysvLK2pra1PaX1NTE2VlZWllzgblSk57uWzH\nDlzXrgAMnT4da25m7UUX5WwGxkI7XkFTruSlk626unqRc+6YTld0zqX1Ai4EHtmtfRnw644+U1FR\n4VJVX1+f8mezSbmS02aupUudGzbMuYaGnOdpVVDHKw8oV/LSyQYsdAnU5UycPF0HDNutPRRYn4Ht\nSrFobvZfDzwQRo+Gnj2DzSNS4DJR2P8CjDSzEWYWAS4GZmZgu1IMbr0VTj4Zdu70BX3GDH+Nuoik\nLO2Tp865ZjObBLwEdAGmOueWp51Mwqulxb/A99KPOspf/dKrV7C5REIiI/dgO+dmAbMysS0JuQ0b\n4OyzKa+uhjFj4KKL/EtEMkZ3nkputPbQ+/eHIUNo6dEj2DwiIabCLtk3fbqfz6WpyT/w4tln2XDq\nqUGnEgktFXbJntZ7JIYMgX333TVnuohklQq7ZF5zM1xyiZ/PBfxDL156Sc8aFckRFXbJnNYeemmp\nnye9W7dg84gUKRV2yYwFC/zNRWvX+vajj8JNmuhTJAgq7CHW2NjItGnTaGxszN5OWnvp++zj53PZ\nsMG3czS3i4h8nZ4lFlKNjY3U1NQQjUaZNm0adXV1VGb6Ac+33ALr1sFDD8Hw4bB4sQq6SB5Qjz2k\nGhoaiMVitLS0EIvFaGhoyPxOtm+HaNRPBwAq6iJ5QoU9pKqqqohEIpSUlBCJRKiqqkp/ox98AKed\nBn/5i29PmQKPPw5duqS/bRHJGBX2kKqsrKSuro7x48dnbhimTx/46CP48EPfVg9dJC9pjD3EKisr\niUaj6RX12lp/5+jTT8Nee8GyZf7uURHJW/oXKh3bsgU+/hg2bfJtFXWRvKd/pfJVTU0wYQI8+6xv\nT5gAr74K/foFm0tEEqbCLl/VvTssXQrvvuvbJSUaSxcpMCrsAo2NcM45/vLF0lLf/vGPg04lIilS\nYRc//LJ0Kbz3nm+X6py6SCHTv+Bi1NICt90Ge+8NkybB6afDW29BJBJ0MhHJABX2YlRSAvPn+6cZ\ntVJRFwkNDcUUizVr4OKL4ZNPfPvpp+GxxwKNJCLZocJeLKJReOUVWLLEt7t3DzaPiGSNhmLC7Omn\n2X/WLKiqgkMO8XO99OwZdCoRyTL12MPs1VfpP38+xGK+raIuUhRU2MNk82a47jpYvty3b7+dxffd\npxOjIkVGhT1MduyAJ56A1rnXe/bUlLoiRUhj7IVuwQJ/hcsdd8CAAbBqlb8+XUSKlnrshe7Pf4Zp\n0/wMjKCiLiIq7AWnuRnuvttfugj+ztG33oLBg4PNJSJ5Q4W90OzcCfffD88959tdu0Lv3sFmEpG8\nosJeCNauheuv9ydHu3Xzwy/33Rd0KhHJUyrshWDRIt9L/+tffXvgQM2RLiLt0lUx+cg5eOEF2LYN\nLrrIz5W+Zg0MGhR0MhEpACrs+epXv/Lj6f/0T753rqIuIgnSUEy+aGqC//gP+PxzX8iffBLq6jTk\nIiJJU2HPF++8Az//Ocya5duDBvkrXkREkpRWYTezX5rZSjNbambPmVmfTAUrCkuXwqOP+uXRo/1d\no5deGmwmESl46fbYZwOHO+eOBN4GfpJ+pCJyzz1w883+JCnA8OGBxhGRcEirsDvnXnbONceb84Gh\n6UcKsZ074eGHYfVq377jDnjjDU2nKyIZlckx9vHAixncXvhs2AD/+q+7hl/694d+/YLNJCKhY865\njlcwewVo61q7yc65GfF1JgPHAN9x7WzQzCYCEwHKy8sramtrUwrc1NREWVlZSp/NpvZyRT77jAGv\nvsr6c84BoOcHH7Bt2LCcXe1SaMcraMqVHOVKXjrZqqurFznnjul0RedcWi9gHNAI9Ez0MxUVFS5V\n9fX1KX82m9rNNWWKc127Ord6dU7ztCq44xUw5UqOciUvnWzAQpdAjU33qpixwI3At51z29LZVqjM\nmePnSQf40Y/8OPqIEcFmEpGike4Y+31Ab2C2mS0xswcykKmwRaNw+eVw222+3b07HHRQoJFEpLik\nNaWAc+4bmQpS0LZvZ/DMmXDKKX72xRdfhAMPDDqViBQp3XmaCTNncvBdd/khGIDDDvM9dRGRAGgS\nsFS9/TasWwdjxsCFF7L40085+rTTgk4lIqLCnrIJE+DTT2H5cigp4YtDDw06kYgIoKGYxDkHv/89\nbN3q2488Ag0NUKJDKCL5RVUpUUuW+IdeTJ3q2wcfDOXlwWYSEWmDCntHPvsM/vAHvzx6tJ8f/dpr\ng80kItIJFfaO/PjHcOGFsHmzb48Zo6EXEcl7qlJ7amyE9ev98n/+J/z5z7D33sFmEhFJggr77jZu\n9L3yW2/17aFD4cgjg80kIpIkFfYdO+Cll/xy//4wcybcfnuwmURE0qDCftddMHYsrFjh26efDnk6\n3aeISCKK8wal99+HL7+EQw6BH/zATwEwalTQqUREMqL4CntzM5x6KowcCbNnQ+/ecNZZQacSEcmY\n4ijszsHcuX72xdJS+O1vNfuiiIRWcYyxP/88VFX5rwDV1bDffoFGEhHJlvAW9i1bYOlSv3zWWfDY\nY3DGGYFGEhHJhfAOxZx/Pqxa5afXLS2FceOCTiQikhPhKuxLl/rJubp1g5/9zH+vNFw/oohIZ8Iz\nFLNypZ+o6777fPv44/1LRKTIFHZh37kTli3zy4ccAg88AOPHB5tJRCRghV3Yb7gBTj7ZP8kI4Kqr\noG/fYDOJiASs4AagIxs3+nnS+/WDa66BE07wc7yIiAhQaD32L77g2CuugMmTfXvkSP9UI7Ngc4mI\n5JHCKux77cW7V18N118fdBIRkbxVWIUd+PjMM+Eb3wg6hohI3iq4wi4iIh1TYRcRCRkVdhGRkFFh\nFxEJGRV2EZGQUWEXEQkZFXYRkZBRYRcRCRkVdhGRkFFhFxEJGRV2EZGQyUhhN7MbzMyZ2YBMbE9E\nRFKXdmE3s2HA6cAH6ccREZF0ZaLHfhfwb4DLwLZERCRN5lzq9djMvg3UOOeuM7P3gGOcc5+2s+5E\nYCJAeXl5RW1tbdL7W758OQsWLOC4447jsMMOSzl3NjQ1NVFWVhZ0jK9RruQoV3KUK3npZKuurl7k\nnDum0xWdcx2+gFeAN9p4nQO8BuwdX+89YEBn23POUVFR4ZI1b94816NHD1dSUuJ69Ojh5s2bl/Q2\nsqm+vj7oCG1SruQoV3KUK3npZAMWugRqbKfPPHXOndbW983sCGAE8Lr5R9MNBRab2XHOuY87/T9K\nkhoaGojFYrS0tBCLxWhoaKCysjLTuxERKXgpP8zaObcM2Ke13dlQTLqqqqqIRCJEo1EikQhVVVXZ\n2I2ISMErmOvYKysrqaurY/z48dTV1am3LiLSjpR77Htyzg3P1LbaU1lZSTQaVVEXEelAwfTYRUQk\nMSrsIiIho8IuIhIyKuwiIiGjwi4iEjIq7CIiIZPWXDEp79RsA/B+ih8fAGTlJqg0KVdylCs5ypWc\nfM0F6WXb3zk3sLOVAins6TCzhS6RSXByTLmSo1zJUa7k5GsuyE02DcWIiISMCruISMgUYmF/KOgA\n7VCu5ChXcpQrOfmaC3KQreDG2EVEpGOF2GMXEZEO5H1hN7NfmtlKM1tqZs+ZWZ921htrZm+Z2Soz\nuykHuS40s+Vm1mJm7Z7hNrP3zGyZmS0xs4V5lCvXx6ufmc02s3fiX/u2s97O+LFaYmYzs5inw5/f\nzLqZ2VPx918zs+HZypJkrsvNbMNux+jKHOWaamafmNkb7bxvZnZvPPdSMzs6T3JVmdnm3Y7XT3OQ\naZiZ1ZvZivi/xevaWCe7xyuRxywF+QK+BZTGl38B/KKNdboA7wIHABHgdeDQLOcaBRwMNOAfMNLe\neu+R4CMDc5UroON1B3BTfPmmtv47xt9rysEx6vTnB64BHogvXww8lSe5Lgfuy9Xv0277PQU4Gnij\nnffPBF4EDDgBeC1PclUBL+T4WA0Gjo4v9wbebuO/Y1aPV9732J1zLzvnmuPN+fhH8O3pOGCVc261\ncy4G1OKfyZrNXCucc29lcx+pSDBXzo9XfPuPx5cfB87N8v46ksjPv3ve6UCNxZ8BGXCuQDjn5gKf\ndbDKOcDvnDcf6GNmg/MgV8455z5yzi2OL28BVgBD9lgtq8cr7wv7Hsbj/y+3pyHA2t3a6/j6gQyK\nA142s0VmNjHoMHFBHK9y59xH4H/x2e2xinvobmYLzWy+mWWr+Cfy8/99nXjHYjPQP0t5kskFcH78\nz/fpZjYsy5kSlc//BivN7HUze9HMDsvljuNDeKOB1/Z4K6vHK2NPUEqHmb0CDGrjrcnOuRnxdSYD\nzcC0tjbRxvfSvtwnkVwJOMk5t97M9gFmm9nKeC8jyFw5P15JbGa/+PE6AJhjZsucc++mm20Pifz8\nWTlGnUhkn88DTzrnomZ2Nf6vijFZzpWIII5XIhbjb8NvMrMzgf8HjMzFjs2sDHgG+KFz7os9327j\nIxk7XnlR2J1zp3X0vpmNA84Galx8gGoP64Ddey5DgfXZzpXgNtbHv35iZs/h/9xOq7BnIFfOj5eZ\n/c3MBjvnPor/yflJO9toPV6rzawB39vJdGFP5OdvXWedmZUCe5P9P/k7zeWc27hb82H8ead8kJXf\nqXTtXlCdc7PM7H4zG+Ccy+o8MmbWFV/Upznnnm1jlawer7wfijGzscCNwLedc9vaWe0vwEgzG2Fm\nEfzJrqxdUZEoM+tlZr1bl/Engts8e59jQRyvmcC4+PI44Gt/WZhZXzPrFl8eAJwEvJmFLIn8/Lvn\nvQCY006nIqe59hiH/TZ+/DYfzAS+H7/a4wRgc+vQW5DMbFDruREzOw5f8zZ2/Km092nAo8AK59yv\n2lktu8crl2eLUzzDvAo/FrUk/mq9UmFfYNYeZ5nfxvfuJucg13n4/+tGgb8BL+2ZC391w+vx1/J8\nyRXQ8eoP1AHvxL/2i3//GOCR+PKJwLL48VoGTMhinq/9/MAt+A4EQHfg6fjv3wLggGwfowRz3Rb/\nXXodqAcOyVGuJ4GPgB3x368JwNXA1fH3DfhNPPcyOrhSLMe5Ju12vOYDJ+Yg08n4YZWlu9WtM3N5\nvHTnqYhIyOT9UIyIiCRHhV1EJGRU2EVEQkaFXUQkZFTYRURCRoVdRCRkVNhFREJGhV1EJGT+P3ZC\nJwj9tpLIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116bf2090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs,ys,'k.')\n",
    "plt.plot(xs, ans[0]*xs+ans[1], 'r:')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Errors\n",
    "\n",
    "In the above example, it looks like the final $\\chi^2$ of our fit was 2.8.  Is this good?\n",
    "- Yes\n",
    "- No\n",
    "- Maybe\n",
    "- Depends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the way we've been writing $\\chi^2$, it scales with the number of measurements we make:\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "To make a fair comparison, we need to normalize this to the error we expect in a \"perfect\" fit.  Suppose we knew $m,b$ perfectly, so that we could subtract $f(x_i,m,b)$ from our measurement $y_i=f(x_i,m,b)+n_i$.  In this case, the residual would be $n_i$, and $\\langle|n_i|^2\\rangle=\\sigma_i^2$.  If we know our $\\sigma^2$ well, we can construct a *reduced* $\\chi^2$,\n",
    "\\begin{equation}\n",
    "\\chi_r^2=\\frac{1}{N}\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}},\n",
    "\\end{equation}\n",
    "where we expect $\\chi_r^2\\approx1$ if we know $\\sigma_i$ well enough.\n",
    "\n",
    "So above, we had $N=5$ measurements, to $\\chi_r^2=\\chi^2/5=0.58$.  That's a good fit.  Perhaps too good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Fits are \"Too Good to be True\"\n",
    "\n",
    "Suppose I have a silly line, $y=0$, and I add some noise to a bunch of measurements of this line, and then fit a line to those measurements and ask for my reduced $\\chi^2$ with perfect knowledge of my noise statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980758577942968\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    m,b = np.polyfit(xs, ys, deg=1)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval([m,b], xs))**2) / ys.size)\n",
    "print(np.average(chisq_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is $\\chi_r^2<1$?\n",
    "- The line ate the noise\n",
    "- Need better noise model\n",
    "- There wasn't actually a line\n",
    "- Dividing by wrong number of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a hint, here's the same code, fitting a 4th order (5 parameter) polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5030894745224488\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    poly = np.polyfit(xs, ys, deg=4)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval(poly, xs))**2) / ys.size)\n",
    "print(np.average(chisq_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the degrees of freedom in your model give it enough wiggle room to absorb noise.  Think if it this way: if I fit a line to 2 noisy points, I'll get a perfect fit ($\\chi^2=0$), but that doesn't mean that my answer is perfect.\n",
    "\n",
    "In fact, for every degree of freedom in our model, we can absorb out a degree of freedom in our data.  That means that when we thought we had 10 independent measurements, we actually only had 8 (or 5 in the `deg=4` case).  To account for this, the *real* definition of the reduced $\\chi^2$ is:\n",
    "\\begin{equation}\n",
    "\\chi_r^2=\\frac{1}{N-M}\\sum_i{\\frac{|y_i-f(\\vec x_i,\\vec p)|^2}{\\sigma_i^2}},\n",
    "\\end{equation}\n",
    "where $M$ is the number of degrees of freedom in the model.  Those degrees of freedom absorb out noise in the data that will then not contribute to the $\\chi^2$.  These degrees of freedom also mean that your model is not as over-constrained as you might otherwise think.  To do good science, we want our model to be highly over-constrained, so that errors beat down as $\\sqrt{N-M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9927387234221549\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    poly = np.polyfit(xs, ys, deg=4)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval(poly, xs))**2) / (ys.size-len(poly)))\n",
    "print(np.average(chisq_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating $\\chi^2$ to Parameter Errors\n",
    "\n",
    "The final step here is to use our $\\chi^2$ to estimate the errors on our parameters.  Why is there error in our parameters, and how did it get there?  Well remember when we said that the degrees of freedom in our model absorbed out some of the noise?  Yep.  That's how.\n",
    "\n",
    "How much did it absorb?  Apparently the difference between $N$ and $N-M$.  A \"perfect\" fit should have had a (non-reduced) $\\chi^2$ that was $M$ higher. That means our $\\chi^2$ (again, *not* reduced) should be allowed to increase by 1 for each parameter we fit to get 1$\\sigma$ error bars (because $\\chi^2$ is in units of $\\sigma$).\n",
    "We change $\\vec p$ until $\\chi^2$ increases by 1 (for 1$\\sigma$ error bars), 4 (for 2$\\sigma$ error bars), or 9 (for 3$\\sigma$ error bars).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0237900858322155\n",
      "1.0242099141677845\n",
      "1.0124092932583366\n",
      "1.0125907067416637\n"
     ]
    }
   ],
   "source": [
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])\n",
    "\n",
    "def chisq_min(p):\n",
    "    m,b = p\n",
    "    return chisq(ys, f(xs,m,b), 1.)\n",
    "\n",
    "chi0 = chisq_min(ans)\n",
    "print(chisq_min(ans+np.array([.32,0])) - chi0)\n",
    "print(chisq_min(ans+np.array([-.32,0])) - chi0)\n",
    "print(chisq_min(ans+np.array([0,0.45])) - chi0)\n",
    "print(chisq_min(ans+np.array([0,-0.45])) - chi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 2.023231 +/- 0.320000\n",
      "b = 0.775493 +/- 0.450000\n"
     ]
    }
   ],
   "source": [
    "print('m = %f +/- %f' % (ans[0], .32))\n",
    "print('b = %f +/- %f' % (ans[1], .45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Guns\n",
    "\n",
    "If you have a linear problem you are trying to solve (or if you can linearize it), you can do *much* better than this iterative mumbo-jumbo.  You can solve it in one shot!  Let's go back to our $y_i = mx_i +b + n_i$ example, but extend it to two dimensions: $z_i = ax_i+by_i+c + n_i$\n",
    "\n",
    "You know what $x_i$ and $y_i$ are (they are the coordinates of your measurement), and you measured $z_i$.  It turns out you can frame the measurements you made as a matrix multiplication:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \\begin{smallmatrix} x_0&y_0&1 \\\\ x_1&y_1&1 \\\\ &\\dots& \\\\ x_i & y_i & 1 \\end{smallmatrix} \\right)\n",
    "\\left( \\begin{smallmatrix} a \\\\ b \\\\ c \\end{smallmatrix}\\right)\n",
    "=\n",
    "\\left( \\begin{smallmatrix} z_0 \\\\ z_1 \\\\ \\dots \\\\ z_i \\end{smallmatrix}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Let's define the first matrix to be $\\mathbf{A}$, the second vector (our parameters to solve for) as $\\vec p$, and our measurements $\\vec z$.  Then the above equation reads:\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\vec p = \\vec z\n",
    "\\end{equation}\n",
    "Because $\\mathbf{A}$ is not a square matrix, it is not generally invertible, but $\\mathbf{A}^\\dagger \\mathbf{A}$ is.  It will be, in this case, a 3x3 matrix.  This means we can re-write the above as:\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^\\dagger\\mathbf{A}\\cdot\\vec p = \\mathbf{A}^\\dagger\\vec z\n",
    "\\end{equation}\n",
    "And then, constructing the matrix inverse $(\\mathbf{A}^\\dagger\\mathbf{A})^{-1}$, and applying to both sides, we have:\n",
    "\\begin{equation}\n",
    "\\vec p = (\\mathbf{A}^\\dagger\\mathbf{A})^{-1}\\mathbf{A}^\\dagger\\vec z\n",
    "\\end{equation}\n",
    "\n",
    "The final flourish is, if not all measurements have the same noise, to do inverse-variance weighting.  If we assume our noise for each measurement is independent, we can write down a noise matrix $\\mathbf{N}$ that is diagonal and has $\\sigma_i^2$ in each row corresponding\n",
    "to the i$^{\\rm th}$ measurement.  Then $\\mathbf{N}^{-1}$ is the inverse variance weighting.\n",
    "Adding that in at the beginning, we can run through the same math to get the final answer:\n",
    "\\begin{equation}\n",
    "\\vec p = (\\mathbf{A}^\\dagger\\mathbf{N}^{-1}\\mathbf{A})^{-1}\\mathbf{A}^\\dagger\\mathbf{N}^{-1}\\vec z\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Return the least-squares solution to a linear matrix equation.\n",
      "\n",
      "    Solves the equation `a x = b` by computing a vector `x` that\n",
      "    minimizes the Euclidean 2-norm `|| b - a x ||^2`.  The equation may\n",
      "    be under-, well-, or over- determined (i.e., the number of\n",
      "    linearly independent rows of `a` can be less than, equal to, or\n",
      "    greater than its number of linearly independent columns).  If `a`\n",
      "    is square and of full rank, then `x` (but for round-off error) is\n",
      "    the \"exact\" solution of the equation.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a : (M, N) array_like\n",
      "        \"Coefficient\" matrix.\n",
      "    b : {(M,), (M, K)} array_like\n",
      "        Ordinate or \"dependent variable\" values. If `b` is two-dimensional,\n",
      "        the least-squares solution is calculated for each of the `K` columns\n",
      "        of `b`.\n",
      "    rcond : float, optional\n",
      "        Cut-off ratio for small singular values of `a`.\n",
      "        For the purposes of rank determination, singular values are treated\n",
      "        as zero if they are smaller than `rcond` times the largest singular\n",
      "        value of `a`.\n",
      "\n",
      "        .. versionchanged:: 1.14.0\n",
      "           If not set, a FutureWarning is given. The previous default\n",
      "           of ``-1`` will use the machine precision as `rcond` parameter,\n",
      "           the new default will use the machine precision times `max(M, N)`.\n",
      "           To silence the warning and use the new default, use ``rcond=None``,\n",
      "           to keep using the old behavior, use ``rcond=-1``.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    x : {(N,), (N, K)} ndarray\n",
      "        Least-squares solution. If `b` is two-dimensional,\n",
      "        the solutions are in the `K` columns of `x`.\n",
      "    residuals : {(1,), (K,), (0,)} ndarray\n",
      "        Sums of residuals; squared Euclidean 2-norm for each column in\n",
      "        ``b - a*x``.\n",
      "        If the rank of `a` is < N or M <= N, this is an empty array.\n",
      "        If `b` is 1-dimensional, this is a (1,) shape array.\n",
      "        Otherwise the shape is (K,).\n",
      "    rank : int\n",
      "        Rank of matrix `a`.\n",
      "    s : (min(M, N),) ndarray\n",
      "        Singular values of `a`.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    LinAlgError\n",
      "        If computation does not converge.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    If `b` is a matrix, then all array results are returned as matrices.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Fit a line, ``y = mx + c``, through some noisy data-points:\n",
      "\n",
      "    >>> x = np.array([0, 1, 2, 3])\n",
      "    >>> y = np.array([-1, 0.2, 0.9, 2.1])\n",
      "\n",
      "    By examining the coefficients, we see that the line should have a\n",
      "    gradient of roughly 1 and cut the y-axis at, more or less, -1.\n",
      "\n",
      "    We can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``\n",
      "    and ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:\n",
      "\n",
      "    >>> A = np.vstack([x, np.ones(len(x))]).T\n",
      "    >>> A\n",
      "    array([[ 0.,  1.],\n",
      "           [ 1.,  1.],\n",
      "           [ 2.,  1.],\n",
      "           [ 3.,  1.]])\n",
      "\n",
      "    >>> m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
      "    >>> print(m, c)\n",
      "    1.0 -0.95\n",
      "\n",
      "    Plot the data along with the fitted line:\n",
      "\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
      "    >>> plt.plot(x, m*x + c, 'r', label='Fitted line')\n",
      "    >>> plt.legend()\n",
      "    >>> plt.show()\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.lstsq.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2.02326413, 0.775513  ]), array([2.8835783]), 2, array([3.16227766, 2.23606798]))\n"
     ]
    }
   ],
   "source": [
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])\n",
    "A = np.array([[-2,1],\n",
    "              [-1,1],\n",
    "              [ 0,1],\n",
    "              [ 1,1],\n",
    "              [ 2,1]])\n",
    "print(np.linalg.lstsq(A, ys, rcond=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
