{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A $\\chi^2$ Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np, pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a linear function $f(x,m,b)=mx+b$ that we have measured at various $x_i$'s.  Our measurements will be noisy, so each measurement $y_i=f(x_i,m,b)+n_i$, where $n_i$ is noise drawn from a distribution that we will assume to be Gaussian.  For now, let's also say that each measurement has statistically identical noise with a known variance of $\\sigma_i^2=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,m,b):\n",
    "    return m*x+b\n",
    "\n",
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "#ys = np.array([-4.84, -1.03, 1.91, 4.83, 4.79], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given the measurements, `ys`, at the coordinates specified in `xs`, which would be your best guess for ($m$,$b$)?\n",
    "- (1,1)\n",
    "- (2,1)\n",
    "- (1,2)\n",
    "- (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a menu of choices, you should just evaluate $\\chi^2$ for each one:\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square: [13.606245099534451, 3.1409625595344508, 20.851115079534452, 10.385832539534452]\n"
     ]
    }
   ],
   "source": [
    "def chisq(ys, mdl, sig):\n",
    "    return np.sum(np.abs(ys-mdl)**2/sig**2)\n",
    "\n",
    "sig = 1. # assume \\sigma_i^2=1\n",
    "ps = [(1,1),(2,1),(1,2),(2,2)]\n",
    "chisqs = [chisq(ys, f(xs,*p), sig) for p in ps]\n",
    "print 'Chi-Square:', chisqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears $(m,b)=(2,1)$ is our best answer!\n",
    "\n",
    "But what happens if you don't have a menu of choices?  Your options are:\n",
    "- make your own menu (e.g. brute force sample a grid at a chosen resolution), or\n",
    "- optimize!\n",
    "What follows is a trip down the rabbit-hole of optimizing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d09a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs,ys,'k.')\n",
    "plt.plot(xs, 2*xs+1, 'r:')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why $\\chi^2$?\n",
    "\n",
    "When we are fitting a model to data, we are looking for the most likely (maximum likelihood) solution.  What is the likelihood of a solution?  It's the product of the probability of\n",
    "getting each data point $y_i$ given the model and the noise statistics for each measurement:\n",
    "\\begin{equation}\n",
    "L = \\prod_i\\frac{1}{\\sigma_i\\sqrt{2\\pi}}\n",
    "e^{-\\left|y_i-f(\\vec c_i,\\vec p)\\right|^2/2\\sigma_i^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Exponentials are cumbersome, so we could just maximize $\\log L$ to the same effect:\n",
    "\\begin{equation}\n",
    "\\log L = -\\frac12\\sum_i\\frac{\\left|y_i-f(\\vec c_i,\\vec p)\\right|^2}{\\sigma_i^2} - \\sum_i \\log(\\sigma_i\\sqrt{2\\pi})\n",
    "\\end{equation}\n",
    "The second term doesn't depend on the model, so it's effectively a constant.  Flipping the negative to a positive, maximizing $\\log L$ is the same as minimizing $\\chi^2$:\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "Moreover, since $L\\propto e^{-\\frac12\\chi^2}$, it's an effective measure of the \"sigmas\" of your fit relative to the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing $\\chi^2$\n",
    "\n",
    "Optimizing (which in numerics is almost always framed as minimizing) a general function can be done by looping over the following 4 steps:\n",
    "1. Start somewhere.  Choose a $\\vec p_j$ and evaluate\n",
    "\\begin{equation}\n",
    "\\chi_j^2=\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p_j)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}\n",
    "2. Compute the gradient (slope) of $\\chi^2$ w.r.t. $p_A,p_B,\\dots=\\vec p$ to get $\\frac{\\partial\\chi^2}{\\partial \\vec p}$\n",
    "3. Check to see if either $\\chi_j^2$ or $\\frac{\\partial\\chi^2}{\\partial \\vec p}$ are small enough to declare victory.  If so, STOP.\n",
    "4. Take a step \"downhill\" toward your target $\\chi^2_T$:\n",
    "\\begin{align}\n",
    "\\frac{\\chi_T^2-\\chi_j^2}{\\vec p_{j+1}-\\vec p_j}&=-\\frac{\\partial\\chi^2}{\\partial \\vec p_j}\\\\\n",
    "\\vec p_{j+1} &= \\vec p_j - \\frac{\\chi_T^2-\\chi_j^2}{\\partial\\chi^2/\\partial \\vec p_j}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a Gradient Numerically\n",
    "\n",
    "Suppose you have a general function $g(\\vec x)$ that you need to take the gradient of:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial g}{\\partial\\vec x}=\\left(\\frac{\\partial g}{\\partial x_0}, \\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, \\dots\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Numerically, this might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(x) = 33.0\n",
      "dg/dx = [  2.00001  64.00048  -1.     ]\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return x[0]**2 + 2*x[1]**4 + np.cos(x[2])\n",
    "\n",
    "x0,x1,x2 = x = np.array([1.,2,np.pi/2])\n",
    "g0 = g(x)\n",
    "#dx = 1e-15 # errors from numerical precision\n",
    "dx = 1e-5 # good?\n",
    "#dx = 1e-1 # errors from step size\n",
    "x0_step = np.array([x0+dx,x1,x2])\n",
    "x1_step = np.array([x0,x1+dx,x2])\n",
    "x2_step = np.array([x0,x1,x2+dx])\n",
    "dg_dx = np.array([(g(x0_step)-g0)/dx, (g(x1_step)-g0)/dx, (g(x2_step)-g0)/dx])\n",
    "print 'g(x) =', g0\n",
    "print 'dg/dx =', dg_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we iterate along, we can eventually find where (or if) the function's gradient goes to zero, indicating we've minimized it.  However, there is something of an art to taking steps of the right size such that:\n",
    "- you don't spend forever getting there, but\n",
    "- you don't overstep, go unstable, and end up at infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize(func, xs, stepsize=1e-2, dx=1e-5, maxiter=100, tol=1e-3, verbose=True):\n",
    "    for iter in xrange(maxiter):\n",
    "        if verbose: print iter, xs\n",
    "        f0 = func(xs)\n",
    "        ndims = len(xs)\n",
    "        x_steps = [xs.copy() for i in xrange(ndims)]\n",
    "        if verbose: print f0\n",
    "        for i in xrange(ndims):\n",
    "            x_steps[i][i] += dx\n",
    "        grad = np.array([(func(x_steps[i])-f0)/dx for i in xrange(ndims)])\n",
    "        if verbose: print grad, np.abs(grad).max()\n",
    "        if np.abs(grad).max() < tol: break\n",
    "        xs = np.array([xs[i]-stepsize*grad[i] for i in xrange(ndims)])\n",
    "    info = {'iter':iter, 'func':f0, 'grad':grad, 'x':xs}\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([ -4.99999997e-06,   4.99659163e-02,   3.14158763e+00]), 'grad': array([  0.00000000e+00,   9.98256000e-04,  -2.62234678e-08]), 'func': -0.99998753401124041, 'iter': 509}\n"
     ]
    }
   ],
   "source": [
    "print minimize(g, np.array([1.,2,3]), verbose=False, stepsize=.03, maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([ 0.1350602 ,  0.24768024,  3.08945107]), 'grad': array([ 0.27067174,  0.12173925, -0.05216506]), 'func': -0.97278242120521097, 'iter': 999}\n"
     ]
    }
   ],
   "source": [
    "print minimize(g, np.array([1.,2,3]), verbose=False, stepsize=.001, maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([  5.11997224e-01,   7.11588315e+15,   3.03826127e+00]), 'grad': array([ 0.,  0.,  0.]), 'func': 5.1279670697028245e+63, 'iter': 4}\n"
     ]
    }
   ],
   "source": [
    "print minimize(g, np.array([1.,2,3]), verbose=False, stepsize=.1, maxiter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are libraries for this kind of thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -1.000000\n",
      "         Iterations: 146\n",
      "         Function evaluations: 259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -4.28491875e-09,  -9.24782387e-05,   3.14159258e+00])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "scipy.optimize.fmin(g, np.array([1.,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Optimizing $\\chi^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.883578\n",
      "         Iterations: 35\n",
      "         Function evaluations: 67\n",
      "m,b = [ 2.02323133  0.77549284]\n"
     ]
    }
   ],
   "source": [
    "def chisq_min(p):\n",
    "    m,b = p\n",
    "    return chisq(ys, f(xs,m,b), 1.) \n",
    "\n",
    "ans = scipy.optimize.fmin(chisq_min, np.array([.5,.5]))\n",
    "print 'm,b =', ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've gone and fit `m` and `b` in our `f(x) = m*x+b` example, and now we have an answer.  Is this the actual answer?  What are the statistical errors on these measurements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you were wondering, the above is what `np.polyfit` does, except it's programmed in the gradients analytically, so it converges faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m,b = 2.023264127 0.775513002\n"
     ]
    }
   ],
   "source": [
    "m,b = np.polyfit(xs,ys, deg=1)\n",
    "print 'm,b =', m,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJwj9tpLIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108e46810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(xs,ys,'k.')\n",
    "plt.plot(xs, ans[0]*xs+ans[1], 'r:')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Errors\n",
    "\n",
    "In the above example, it looks like the final $\\chi^2$ of our fit was 2.8.  Is this good?\n",
    "- Yes\n",
    "- No\n",
    "- Maybe\n",
    "- Depends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the way we've been writing $\\chi^2$, it scales with the number of measurements we make:\n",
    "\\begin{equation}\n",
    "\\chi^2=\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "To make a fair comparison, we need to normalize this to the error we expect in a \"perfect\" fit.  Suppose we knew $m,b$ perfectly, so that we could subtract $f(x_i,m,b)$ from our measurement $y_i=f(x_i,m,b)+n_i$.  In this case, the residual would be $n_i$, and $\\langle|n_i|^2\\rangle=\\sigma_i^2$.  If we know our $\\sigma^2$ well, we can construct a *reduced* $\\chi^2$,\n",
    "\\begin{equation}\n",
    "\\chi_r^2=\\frac{1}{N}\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}},\n",
    "\\end{equation}\n",
    "where we expect $\\chi_r^2\\approx1$ if we know $\\sigma_i$ well enough.\n",
    "\n",
    "So above, we had $N=5$ measurements, to $\\chi_r^2=\\chi^2/5=0.58$.  That's a good fit.  Perhaps too good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Fits are \"Too Good to be True\"\n",
    "\n",
    "Suppose I have a silly line, $y=0$, and I add some noise to a bunch of measurements of this line, and then fit a line to those measurements and ask for my reduced $\\chi^2$ with perfect knowledge of my noise statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801600795372\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    m,b = np.polyfit(xs, ys, deg=1)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval([m,b], xs))**2) / ys.size)\n",
    "print np.average(chisq_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is $\\chi_r^2<1$?\n",
    "- The line ate the noise\n",
    "- Need better noise model\n",
    "- There wasn't actually a line\n",
    "- Dividing by wrong number of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a hint, here's the same code, fitting a 4th order (5 parameter) polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500343873777\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    poly = np.polyfit(xs, ys, deg=4)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval(poly, xs))**2) / ys.size)\n",
    "print np.average(chisq_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the degrees of freedom in your model give it enough wiggle room to absorb noise.  Think if it this way: if I fit a line to 2 noisy points, I'll get a perfect fit ($\\chi^2=0$), but that doesn't mean that my answer is perfect.\n",
    "\n",
    "In fact, for every degree of freedom in our model, we can absorb out a degree of freedom in our data.  That means that when we thought we had 10 independent measurements, we actually only had 8 (or 5 in the `deg=4` case).  To account for this, the *real* definition of the reduced $\\chi^2$ is:\n",
    "\\begin{equation}\n",
    "\\chi_r^2=\\frac{1}{N-M}\\sum_i{\\frac{|y_i-f(\\vec c_i,\\vec p)|^2}{\\sigma_i^2}},\n",
    "\\end{equation}\n",
    "where $M$ is the number of degrees of freedom in the model.  Those degrees of freedom absorb out noise in the data that will then not contribute to the $\\chi^2$.  These degrees of freedom also mean that your model is not as over-constrained as you might otherwise think.  To do good science, we want our model to be highly over-constrained, so that errors beat down as $\\sqrt{N-M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992700014038\n"
     ]
    }
   ],
   "source": [
    "chisq_r = []\n",
    "for i in xrange(10000):\n",
    "    ys = np.random.normal(scale=1., size=10)\n",
    "    xs = np.arange(ys.size) # invent a dummy x variable\n",
    "    poly = np.polyfit(xs, ys, deg=4)\n",
    "    chisq_r.append(np.sum(np.abs(ys - np.polyval(poly, xs))**2) / (ys.size-len(poly)))\n",
    "print np.average(chisq_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating $\\chi^2$ to Parameter Errors\n",
    "\n",
    "The final step here is to use our $\\chi^2$ to estimate the errors on our parameters.  Why is there error in our parameters, and how did it get there?  Well remember when we said that the degrees of freedom in our model absorbed out some of the noise?  Yep.  That's how.\n",
    "\n",
    "How much did it absorb?  Apparently the difference between $N$ and $N-M$.  A \"perfect\" fit should have had a (non-reduced) $\\chi^2$ that was $M$ higher. That means our $\\chi^2$ (again, *not* reduced) should be allowed to increase by 1 for each parameter we fit to get 1$\\sigma$ error bars (because $\\chi^2$ is in units of $\\sigma$).\n",
    "We change $\\vec p$ until $\\chi^2$ increases by 1 (for 1$\\sigma$ error bars), 4 (for 2$\\sigma$ error bars), or 9 (for 3$\\sigma$ error bars).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02379008583\n",
      "1.02420991417\n",
      "1.01240929326\n",
      "1.01259070674\n"
     ]
    }
   ],
   "source": [
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])\n",
    "\n",
    "def chisq_min(p):\n",
    "    m,b = p\n",
    "    return chisq(ys, f(xs,m,b), 1.)\n",
    "\n",
    "chi0 = chisq_min(ans)\n",
    "print chisq_min(ans+np.array([.32,0])) - chi0\n",
    "print chisq_min(ans+np.array([-.32,0])) - chi0\n",
    "print chisq_min(ans+np.array([0,0.45])) - chi0\n",
    "print chisq_min(ans+np.array([0,-0.45])) - chi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 2.023231 +/- 0.320000\n",
      "b = 0.775493 +/- 0.450000\n"
     ]
    }
   ],
   "source": [
    "print 'm = %f +/- %f' % (ans[0], .32)\n",
    "print 'b = %f +/- %f' % (ans[1], .45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Guns\n",
    "\n",
    "If you have a linear problem you are trying to solve (or if you can linearize it), you can do *much* better than this iterative mumbo-jumbo.  You can solve it in one shot!  Let's go back to our $y_i = mx_i +b + n_i$ example, but extend it to two dimensions: $z_i = ax_i+by_i+c + n_i$\n",
    "\n",
    "You know what $x_i$ and $y_i$ are (they are the coordinates of your measurement), and you measured $z_i$.  It turns out you can frame the measurements you made as a matrix multiplication:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \\begin{smallmatrix} x_0&y_0&1 \\\\ x_1&y_1&1 \\\\ &\\dots& \\\\ x_i & y_i & 1 \\end{smallmatrix} \\right)\n",
    "\\left( \\begin{smallmatrix} a \\\\ b \\\\ c \\end{smallmatrix}\\right)\n",
    "=\n",
    "\\left( \\begin{smallmatrix} z_0 \\\\ z_1 \\\\ \\dots \\\\ z_i \\end{smallmatrix}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Let's define the first matrix to be $\\mathbf{A}$, the second vector (our parameters to solve for) as $\\vec p$, and our measurements $\\vec z$.  Then the above equation reads:\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\vec p = \\vec z\n",
    "\\end{equation}\n",
    "Because $\\mathbf{A}$ is not a square matrix, it is not generally invertible, but $\\mathbf{A}^\\dagger \\mathbf{A}$ is.  It will be, in this case, a 3x3 matrix.  This means we can re-write the above as:\n",
    "\\begin{equation}\n",
    "\\mathbf{A}^\\dagger\\mathbf{A}\\cdot\\vec p = \\mathbf{A}^\\dagger\\vec z\n",
    "\\end{equation}\n",
    "And then, constructing the matrix inverse $(\\mathbf{A}^\\dagger\\mathbf{A})^{-1}$, and applying to both sides, we have:\n",
    "\\begin{equation}\n",
    "\\vec p = (\\mathbf{A}^\\dagger\\mathbf{A})^{-1}\\mathbf{A}^\\dagger\\vec z\n",
    "\\end{equation}\n",
    "\n",
    "The final flourish is, if not all measurements have the same noise, to do inverse-variance weighting.  If we assume our noise for each measurement is independent, we can write down a noise matrix $\\mathbf{N}$ that is diagonal and has $\\sigma_i^2$ in each row corresponding\n",
    "to the i$^{\\rm th}$ measurement.  Then $\\mathbf{N}^{-1}$ is the inverse variance weighting.\n",
    "Adding that in at the beginning, we can run through the same math to get the final answer:\n",
    "\\begin{equation}\n",
    "\\vec p = (\\mathbf{A}^\\dagger\\mathbf{N}^{-1}\\mathbf{A})^{-1}\\mathbf{A}^\\dagger\\mathbf{N}^{-1}\\vec z\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Return the least-squares solution to a linear matrix equation.\n",
      "\n",
      "    Solves the equation `a x = b` by computing a vector `x` that\n",
      "    minimizes the Euclidean 2-norm `|| b - a x ||^2`.  The equation may\n",
      "    be under-, well-, or over- determined (i.e., the number of\n",
      "    linearly independent rows of `a` can be less than, equal to, or\n",
      "    greater than its number of linearly independent columns).  If `a`\n",
      "    is square and of full rank, then `x` (but for round-off error) is\n",
      "    the \"exact\" solution of the equation.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a : (M, N) array_like\n",
      "        \"Coefficient\" matrix.\n",
      "    b : {(M,), (M, K)} array_like\n",
      "        Ordinate or \"dependent variable\" values. If `b` is two-dimensional,\n",
      "        the least-squares solution is calculated for each of the `K` columns\n",
      "        of `b`.\n",
      "    rcond : float, optional\n",
      "        Cut-off ratio for small singular values of `a`.\n",
      "        For the purposes of rank determination, singular values are treated\n",
      "        as zero if they are smaller than `rcond` times the largest singular\n",
      "        value of `a`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    x : {(N,), (N, K)} ndarray\n",
      "        Least-squares solution. If `b` is two-dimensional,\n",
      "        the solutions are in the `K` columns of `x`.\n",
      "    residuals : {(), (1,), (K,)} ndarray\n",
      "        Sums of residuals; squared Euclidean 2-norm for each column in\n",
      "        ``b - a*x``.\n",
      "        If the rank of `a` is < N or M <= N, this is an empty array.\n",
      "        If `b` is 1-dimensional, this is a (1,) shape array.\n",
      "        Otherwise the shape is (K,).\n",
      "    rank : int\n",
      "        Rank of matrix `a`.\n",
      "    s : (min(M, N),) ndarray\n",
      "        Singular values of `a`.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    LinAlgError\n",
      "        If computation does not converge.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    If `b` is a matrix, then all array results are returned as matrices.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Fit a line, ``y = mx + c``, through some noisy data-points:\n",
      "\n",
      "    >>> x = np.array([0, 1, 2, 3])\n",
      "    >>> y = np.array([-1, 0.2, 0.9, 2.1])\n",
      "\n",
      "    By examining the coefficients, we see that the line should have a\n",
      "    gradient of roughly 1 and cut the y-axis at, more or less, -1.\n",
      "\n",
      "    We can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``\n",
      "    and ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:\n",
      "\n",
      "    >>> A = np.vstack([x, np.ones(len(x))]).T\n",
      "    >>> A\n",
      "    array([[ 0.,  1.],\n",
      "           [ 1.,  1.],\n",
      "           [ 2.,  1.],\n",
      "           [ 3.,  1.]])\n",
      "\n",
      "    >>> m, c = np.linalg.lstsq(A, y)[0]\n",
      "    >>> print(m, c)\n",
      "    1.0 -0.95\n",
      "\n",
      "    Plot the data along with the fitted line:\n",
      "\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
      "    >>> plt.plot(x, m*x + c, 'r', label='Fitted line')\n",
      "    >>> plt.legend()\n",
      "    >>> plt.show()\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print np.linalg.lstsq.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 2.02326413,  0.775513  ]), array([ 2.8835783]), 2, array([ 3.16227766,  2.23606798]))\n"
     ]
    }
   ],
   "source": [
    "xs = np.array([-2, -1, 0, 1, 2], dtype=np.float)\n",
    "ys = np.array([-4.0761021 , -0.61376301,  0.96543424,  3.7373177 ,  3.86467818])\n",
    "A = np.array([[-2,1],\n",
    "              [-1,1],\n",
    "              [ 0,1],\n",
    "              [ 1,1],\n",
    "              [ 2,1]])\n",
    "print np.linalg.lstsq(A, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
